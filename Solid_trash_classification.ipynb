{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the data\n",
    "DATADIR = \".//Data//solid_classification\"\n",
    "CATEGORIES = [\"Cardboard\", \"Glass\",\"Metal\",\"Paper\",\"Plastic\"]\n",
    "for category in CATEGORIES:\n",
    "# create path to cars and trucks\n",
    "    path = os.path.join(DATADIR,category)  \n",
    "# iterate over each image per cars and trucks\n",
    "    for img in os.listdir(path):\n",
    "# convert to array\n",
    "        img_array = cv2.imread(os.path.join(path,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create training data\n",
    "IMG_SIZE = 80\n",
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 999/999 [00:00<00:00, 1190.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1279.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 999/999 [00:00<00:00, 1212.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1116.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 999/999 [00:00<00:00, 1209.95it/s]\n"
     ]
    }
   ],
   "source": [
    "## create traing data\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:  # do cars and trucks\n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to cars and trucks\n",
    "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=solid 1=wet\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image cars and trucks\n",
    "            try:\n",
    "        # convert to array\n",
    "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)  \n",
    "                # resize to normalize data size\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create features and labels\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X_solid = []\n",
    "y_solid = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X_solid.append(features)\n",
    "    y_solid.append(label)\n",
    "\n",
    "X_solid = np.array(X_solid).reshape(-1,\n",
    "                                    IMG_SIZE, IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##write the pickle file\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X_solid.pickle\",\"wb\")\n",
    "pickle.dump(X_solid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y_solid.pickle\",\"wb\")\n",
    "pickle.dump(y_solid, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##divide training and testing dataset\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "# -------------Load X--------------\n",
    "pickle_in = open(\"X_solid.pickle\",\"rb\") \n",
    "train_images = pickle.load(pickle_in)\n",
    "# Split part of X for testing purpose\n",
    "test_images = train_images[int(0.9 * len(train_images)):]\n",
    "train_images = train_images[:int(0.9 * len(train_images))]\n",
    "\n",
    "# -------------Load y--------------\n",
    "pickle_in = open(\"y_solid.pickle\",\"rb\") \n",
    "train_labels = pickle.load(pickle_in)\n",
    "#Split part of Y for testing purpose\n",
    "test_labels = train_labels[int(0.9 * len(train_labels)):]\n",
    "train_labels = numpy.asarray(train_labels[:int(0.9 * len(train_labels))])\n",
    "\n",
    "# convert list to ndarray, and reshape the labels\n",
    "test_labels = numpy.array(test_labels).reshape(len(test_labels),1)\n",
    "train_labels = numpy.array(train_labels).reshape(len(train_labels),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images - Type: <class 'numpy.ndarray'>, Shape (4496, 80, 80, 1)\n",
      "test_images - Type: <class 'numpy.ndarray'>, Shape (500, 80, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_images - Type: {type(train_images)}, Shape {train_images.shape}\")\n",
    "print(f\"test_images - Type: {type(test_images)}, Shape {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[206],\n",
       "         [205],\n",
       "         [204],\n",
       "         ...,\n",
       "         [149],\n",
       "         [149],\n",
       "         [147]],\n",
       "\n",
       "        [[204],\n",
       "         [205],\n",
       "         [204],\n",
       "         ...,\n",
       "         [147],\n",
       "         [147],\n",
       "         [146]],\n",
       "\n",
       "        [[203],\n",
       "         [203],\n",
       "         [204],\n",
       "         ...,\n",
       "         [145],\n",
       "         [145],\n",
       "         [144]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[198],\n",
       "         [198],\n",
       "         [197],\n",
       "         ...,\n",
       "         [173],\n",
       "         [175],\n",
       "         [174]],\n",
       "\n",
       "        [[200],\n",
       "         [199],\n",
       "         [197],\n",
       "         ...,\n",
       "         [175],\n",
       "         [177],\n",
       "         [176]],\n",
       "\n",
       "        [[200],\n",
       "         [199],\n",
       "         [196],\n",
       "         ...,\n",
       "         [176],\n",
       "         [178],\n",
       "         [177]]],\n",
       "\n",
       "\n",
       "       [[[210],\n",
       "         [208],\n",
       "         [210],\n",
       "         ...,\n",
       "         [158],\n",
       "         [149],\n",
       "         [152]],\n",
       "\n",
       "        [[208],\n",
       "         [210],\n",
       "         [203],\n",
       "         ...,\n",
       "         [151],\n",
       "         [153],\n",
       "         [154]],\n",
       "\n",
       "        [[208],\n",
       "         [204],\n",
       "         [203],\n",
       "         ...,\n",
       "         [153],\n",
       "         [152],\n",
       "         [154]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  9],\n",
       "         [ 12],\n",
       "         [ 18],\n",
       "         ...,\n",
       "         [104],\n",
       "         [107],\n",
       "         [107]],\n",
       "\n",
       "        [[  8],\n",
       "         [ 13],\n",
       "         [ 18],\n",
       "         ...,\n",
       "         [105],\n",
       "         [110],\n",
       "         [109]],\n",
       "\n",
       "        [[ 10],\n",
       "         [ 16],\n",
       "         [ 18],\n",
       "         ...,\n",
       "         [104],\n",
       "         [107],\n",
       "         [101]]],\n",
       "\n",
       "\n",
       "       [[[161],\n",
       "         [169],\n",
       "         [173],\n",
       "         ...,\n",
       "         [166],\n",
       "         [174],\n",
       "         [166]],\n",
       "\n",
       "        [[169],\n",
       "         [169],\n",
       "         [172],\n",
       "         ...,\n",
       "         [174],\n",
       "         [168],\n",
       "         [170]],\n",
       "\n",
       "        [[159],\n",
       "         [172],\n",
       "         [170],\n",
       "         ...,\n",
       "         [169],\n",
       "         [173],\n",
       "         [177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[176],\n",
       "         [175],\n",
       "         [180],\n",
       "         ...,\n",
       "         [180],\n",
       "         [176],\n",
       "         [171]],\n",
       "\n",
       "        [[176],\n",
       "         [172],\n",
       "         [179],\n",
       "         ...,\n",
       "         [172],\n",
       "         [168],\n",
       "         [174]],\n",
       "\n",
       "        [[165],\n",
       "         [175],\n",
       "         [169],\n",
       "         ...,\n",
       "         [172],\n",
       "         [168],\n",
       "         [166]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[200],\n",
       "         [199],\n",
       "         [199],\n",
       "         ...,\n",
       "         [182],\n",
       "         [182],\n",
       "         [182]],\n",
       "\n",
       "        [[200],\n",
       "         [199],\n",
       "         [199],\n",
       "         ...,\n",
       "         [182],\n",
       "         [182],\n",
       "         [182]],\n",
       "\n",
       "        [[198],\n",
       "         [199],\n",
       "         [199],\n",
       "         ...,\n",
       "         [181],\n",
       "         [179],\n",
       "         [180]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[197],\n",
       "         [198],\n",
       "         [197],\n",
       "         ...,\n",
       "         [162],\n",
       "         [162],\n",
       "         [162]],\n",
       "\n",
       "        [[199],\n",
       "         [198],\n",
       "         [198],\n",
       "         ...,\n",
       "         [161],\n",
       "         [163],\n",
       "         [163]],\n",
       "\n",
       "        [[201],\n",
       "         [198],\n",
       "         [198],\n",
       "         ...,\n",
       "         [161],\n",
       "         [163],\n",
       "         [163]]],\n",
       "\n",
       "\n",
       "       [[[111],\n",
       "         [113],\n",
       "         [110],\n",
       "         ...,\n",
       "         [159],\n",
       "         [206],\n",
       "         [180]],\n",
       "\n",
       "        [[110],\n",
       "         [111],\n",
       "         [114],\n",
       "         ...,\n",
       "         [169],\n",
       "         [196],\n",
       "         [183]],\n",
       "\n",
       "        [[108],\n",
       "         [109],\n",
       "         [116],\n",
       "         ...,\n",
       "         [202],\n",
       "         [204],\n",
       "         [193]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[197],\n",
       "         [210],\n",
       "         [200],\n",
       "         ...,\n",
       "         [171],\n",
       "         [183],\n",
       "         [  3]],\n",
       "\n",
       "        [[197],\n",
       "         [210],\n",
       "         [202],\n",
       "         ...,\n",
       "         [ 51],\n",
       "         [  7],\n",
       "         [  3]],\n",
       "\n",
       "        [[197],\n",
       "         [211],\n",
       "         [204],\n",
       "         ...,\n",
       "         [  2],\n",
       "         [  3],\n",
       "         [  3]]],\n",
       "\n",
       "\n",
       "       [[[240],\n",
       "         [240],\n",
       "         [242],\n",
       "         ...,\n",
       "         [237],\n",
       "         [248],\n",
       "         [242]],\n",
       "\n",
       "        [[235],\n",
       "         [221],\n",
       "         [192],\n",
       "         ...,\n",
       "         [ 60],\n",
       "         [231],\n",
       "         [244]],\n",
       "\n",
       "        [[237],\n",
       "         [237],\n",
       "         [236],\n",
       "         ...,\n",
       "         [ 61],\n",
       "         [239],\n",
       "         [247]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[183],\n",
       "         [183],\n",
       "         [183],\n",
       "         ...,\n",
       "         [176],\n",
       "         [204],\n",
       "         [187]],\n",
       "\n",
       "        [[182],\n",
       "         [182],\n",
       "         [182],\n",
       "         ...,\n",
       "         [217],\n",
       "         [204],\n",
       "         [189]],\n",
       "\n",
       "        [[181],\n",
       "         [181],\n",
       "         [181],\n",
       "         ...,\n",
       "         [179],\n",
       "         [182],\n",
       "         [188]]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import package\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixRandomSeed in action\n"
     ]
    }
   ],
   "source": [
    "## set the randomseed\n",
    "from DL_tools import fixRandomSeed as frs\n",
    "frs.fixRandomSeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_solid = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(train_images.shape[1:])),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model\n",
    "model_solid.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 25s 169ms/step - loss: 1.5485 - accuracy: 0.2627 - val_loss: 1.3786 - val_accuracy: 0.4420\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 25s 174ms/step - loss: 1.3226 - accuracy: 0.4641 - val_loss: 1.2587 - val_accuracy: 0.4980\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 24s 173ms/step - loss: 1.2056 - accuracy: 0.5181 - val_loss: 1.1673 - val_accuracy: 0.5080\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 25s 174ms/step - loss: 1.1040 - accuracy: 0.5503 - val_loss: 1.0993 - val_accuracy: 0.5600\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 24s 172ms/step - loss: 0.9692 - accuracy: 0.6237 - val_loss: 0.9510 - val_accuracy: 0.6200\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 25s 177ms/step - loss: 0.8685 - accuracy: 0.6657 - val_loss: 1.0717 - val_accuracy: 0.5900\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 24s 173ms/step - loss: 0.7934 - accuracy: 0.6932 - val_loss: 0.9273 - val_accuracy: 0.6160\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 24s 173ms/step - loss: 0.7255 - accuracy: 0.7339 - val_loss: 0.9152 - val_accuracy: 0.6420\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 27s 190ms/step - loss: 0.6359 - accuracy: 0.7563 - val_loss: 0.9667 - val_accuracy: 0.6420\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 24s 172ms/step - loss: 0.5888 - accuracy: 0.7806 - val_loss: 0.9160 - val_accuracy: 0.6460\n"
     ]
    }
   ],
   "source": [
    "history = model_solid.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 - 1s - loss: 0.9160 - accuracy: 0.6460\n",
      "Loss = 0.92, Acc = 0.65\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model_solid.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(f\"Loss = {round(test_loss,2)}, Acc = {round(test_acc,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#打开摄像头\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "    # 读取图片\n",
    "    ret, frame = cap.read()\n",
    "    # 展示获取的图片\n",
    "    cv2.imshow(\"capture\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # 图片写入文件\n",
    "        cv2.imwrite(\"photo.jpg\", frame)\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = cv2.imread(\"photo.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "new_photo = cv2.resize(photo, (IMG_SIZE, IMG_SIZE))\n",
    "new_photo = new_photo.reshape(-1,IMG_SIZE, IMG_SIZE, 1)\n",
    "new_photo = new_photo / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict result is: Glass\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict result is:\",CATEGORIES[model_solid.predict_classes(new_photo)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00134905, 0.54679245, 0.28230035, 0.0059488 , 0.16360942]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
